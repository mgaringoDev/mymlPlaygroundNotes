{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REF:\n",
    "- articles\n",
    "    - [1](https://towardsdatascience.com/linear-regression-from-scratch-cd0dee067f72)\n",
    "    - [2](https://towardsdatascience.com/linear-regression-from-scratch-with-numpy-implementation-finally-8e617d8e274c)\n",
    "    \n",
    "- videos\n",
    "    - [Coding Train](https://www.youtube.com/watch?v=L-Lsfu4ab74)\n",
    "    - [Linear Regression and Multiple Regression](https://www.youtube.com/watch?v=K_EH2abOp00)\n",
    "    \n",
    "    \n",
    "TODO:\n",
    "- Gradient Descent\n",
    "    - [Linear regression gradient descent](https://www.youtube.com/watch?v=WnqQrPNYz5Q)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that linear regression fits a model in the form:\n",
    "\n",
    "$$ y=mx+b $$\n",
    "\n",
    "or \n",
    "\n",
    "$$y=a0+a1x1+a2x2+a3x3+...+anxn$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways of finding these coefficients by let us use **Ordinary Least Mean Square** approach.  The goal is finding a function with coefficients that minimize the error of our model. This line is the best fit that passes through most of the scatter points and also reduces error which is the distance from the point to the line itself.\n",
    "\n",
    "The total error of the model is the sum of the error of each point\n",
    "\n",
    "$$\\sum_{n}^{i=1}r_i^2$$\n",
    "\n",
    "where **r** are the distances between the line and the ith point.\n",
    "\n",
    "The squaring procedure is to remove the ambiguity of the points being above or below the decision line.  We need to minimize the error or distances between the point and the line.\n",
    "\n",
    "$$\\beta_i = \\frac{\\sum_{i}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i}(x_i-\\bar{x})^2} = m$$\n",
    "\n",
    "This gets the slope of the regression line.  To find the bias you need to place it in equation 1 with x and y being the mean of the data points and solving for the bias.\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\bar{\\beta}\\bar{x}$$\n",
    "OR\n",
    "$$bias = \\bar{y} - m\\bar{x}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the previous section it was easier to see how the algorithm works each step and how they are calculated.  In this section we will use matrices instead and dot products to make our calculations easier and faster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is called **Boston house prices dataset** from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about the dataset can be found [here](https://scikit-learn.org/stable/datasets/index.html#boston-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Number of Instances | 506 |\n",
    "|----------------------|---------------------------------------------------------------------------------------|\n",
    "| Number of Attributes | 13 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features\n",
    "- CRIM per capita crime rate by town\n",
    "- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS proportion of non-retail business acres per town\n",
    "- CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX nitric oxides concentration (parts per 10 million)\n",
    "- RM average number of rooms per dwelling\n",
    "- AGE proportion of owner-occupied units built prior to 1940\n",
    "- DIS weighted distances to five Boston employment centres\n",
    "- RAD index of accessibility to radial highways\n",
    "- TAX full-value property-tax rate per 10,000\n",
    "- PTRATIO pupil-teacher ratio by town\n",
    "- B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- LSTAT % lower status of the population\n",
    "- MEDV Median value of owner-occupied homes in 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in our dataset is: 506 and contains 13 features\n",
      "X type =<class 'numpy.ndarray'>\n",
      "y type = <class 'numpy.ndarray'> \n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "\n",
    "# Features of the houses\n",
    "X = dataset.data\n",
    "# Price of the houses\n",
    "y = dataset.target[:,np.newaxis]\n",
    "\n",
    "print(\"Total samples in our dataset is: {} and contains {} features\".format(X.shape[0],X.shape[1]))\n",
    "print(\"X type ={}\\ny type = {} \".format(type(X),type(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset are numpy arrays so let us treat them that way instead of python lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Algo Matrix Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notes we use the RSS for the computation of residuals.  This makes the derivate calculations a little bit easier to see.  But for the purpose of this demonstration we use the MSE similar to the linear regression example.  We can take this further and also use the coefficient of Determination.  These are just various cost functions to identify the residuals.  Its these residuals we want to minimize using gradient decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a general residual function\n",
    "def calcResiduals(X,y,B,costType='mse'):\n",
    "    args = {'features':X,'outputs':y,'coefficients':B}\n",
    "    if costType == 'mse':\n",
    "        residuals = calcMSE(args)\n",
    "    else:\n",
    "        print('So cost type = {}'.format(costType))\n",
    "        return False\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMSE(argsMSE):\n",
    "    # get variables\n",
    "    X = argsMSE['features']\n",
    "    y = argsMSE['outputs']\n",
    "    B = argsMSE['coefficients']\n",
    "    \n",
    "    # by applying the dot product we are essesntially implying regression\n",
    "    # note that we don't have to add b0 here for the bias like in the previous \n",
    "    # example rather we are placing that in the X as its own coloumns\n",
    "    n = len(y)\n",
    "    yHat = X.dot(B)\n",
    "    \n",
    "    mse = (1/(2*n))*np.sum((y-yHat)**2)\n",
    "    \n",
    "    return mse    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the notes for a more detailed derivation from MSE to the DJ.\n",
    "\n",
    "Recall that \n",
    "\n",
    "$ MSE = J(\\beta) = \\frac{1}{n}\\sum_{n}(y_n-\\hat{y}_n)^2$\n",
    "\n",
    "Where $(y_n-\\hat{y}_n)$ are residuals $e$ and $J$ is some cost function.  In this case it was the MSE\n",
    "\n",
    "If we take the derivative with respect to the parameters $B$ we can take the negative step towards minimizing the the residual or cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, B, learningRate=1, n_iters=1,costType='mse'):    \n",
    "    nSamples = len(y)\n",
    "    Jhistory = np.zeros((n_iters,1))\n",
    "    Jcurrent = 0    \n",
    "    Jpreviouse = 0\n",
    "    \n",
    "    # go through each iteration of gradient decent for a specific number of iterations or until the differences between \n",
    "    # each subsequent gradient is small enough to say that it has stablized.\n",
    "    for i in range(n_iters):\n",
    "        # calculate residuals\n",
    "        e = y-np.dot(X,B)\n",
    "        # calculate DJ or gradient of the cost function MSE\n",
    "        DJ = -2/nSamples * np.dot(X.T,e)        \n",
    "        # update B by taking a step in the negative DJ direction.  Note step here is the learning rate\n",
    "        B = B - learningRate*DJ\n",
    "        \n",
    "        # calculate and store the gradient history               \n",
    "        #B = B - (learningRate/nSamples) * X.T.dot((X.dot(B) - y)) \n",
    "        Jhistory[i] = calcResiduals(X, y, B)\n",
    "        if i>1:            \n",
    "            diff = Jhistory[i]-Jhistory[i-1]\n",
    "            diff = np.abs(diff)            \n",
    "            if(diff[0]<0.001):                                 \n",
    "                print('Found stability at iteration:{}'.format(i))\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return (Jhistory, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of data\n",
    "nSamples  = len(y)\n",
    "\n",
    "# Get normalization\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "X = (X-mu) / sigma \n",
    "# You add a coloumn of ones here for the bias term.  \n",
    "# In the notes this is because it should be a nx(p+1) matrix\n",
    "X = np.hstack((np.ones((nSamples,1)),X))\n",
    "\n",
    "# Get number of features\n",
    "n_features = np.size(X,1)\n",
    "# intialize the parameters to 0\n",
    "# these are the coefficients of the model to find\n",
    "params = np.zeros((n_features,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found stability at iteration:360\n"
     ]
    }
   ],
   "source": [
    "n_iters = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = calcResiduals(X, y, params,costType='mse')\n",
    "\n",
    "(Jhistory, optimalparams) = gradientDescent(X, y, params, \n",
    "                                              learningRate=0.01, \n",
    "                                              n_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(Jhistory)), Jhistory, 'r')\n",
    "\n",
    "plt.title(\"Convergence Graph of Cost Function\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost is:  296.0734584980237 \n",
      "\n",
      "Optimal parameters are: \n",
      " [[22.51748033]\n",
      " [-0.73015917]\n",
      " [ 0.70595264]\n",
      " [-0.33187158]\n",
      " [ 0.75607696]\n",
      " [-1.33223785]\n",
      " [ 3.00377771]\n",
      " [-0.13608087]\n",
      " [-2.49080621]\n",
      " [ 1.19738595]\n",
      " [-0.69184228]\n",
      " [-1.90485269]\n",
      " [ 0.88784068]\n",
      " [-3.57220119]] \n",
      "\n",
      "Final cost is:[11.17995531].  Note that the gradient decent exited early due to stability of the cost \n"
     ]
    }
   ],
   "source": [
    "print(\"Initial cost is: \", initial_cost, \"\\n\")\n",
    "\n",
    "print(\"Optimal parameters are: \\n\", optimalparams, \"\\n\")\n",
    "\n",
    "print(\"Final cost is:{}.  Note that the gradient decent exited early due to stability of the cost \".format(Jhistory[360]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlPlaygroundPy36",
   "language": "python",
   "name": "mlplaygroundpy36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
